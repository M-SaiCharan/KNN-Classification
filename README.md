
# ðŸ§  K-Nearest Neighbors (KNN) Classification

This project demonstrates a complete implementation of the **K-Nearest Neighbors (KNN)** algorithm using the **Iris dataset**. The objective is to understand KNN for classification, evaluate its performance across different values of `k`, and visualize decision boundaries.

## ðŸš€ Features

- Uses **Iris dataset** for multi-class classification
- Normalizes features using `StandardScaler`
- Trains KNN models for various values of `k` (1, 3, 5, 7)
- Evaluates models using:
  - Accuracy score
  - Confusion matrix
- Visualizes decision boundaries for `k=5` using 2D plots

## ðŸ“¦ Requirements

Install the required packages using:

```bash
pip install -r requirements.txt
```

**`requirements.txt`**
```txt
numpy
pandas
scikit-learn
matplotlib
```

## ðŸ§ª How to Run

```bash
python knn_classifier.py
```

## ðŸ“Š Output

- Console output showing **accuracy** and **confusion matrix** for different `k` values.
- A **plot of decision boundaries** when `k=5` using the first two features.

## ðŸ“ Project Structure

```
â”œâ”€â”€ knn_classifier.py     # Main Python script
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ requirements.txt      # Dependencies
```

## ðŸ“˜ Dataset Info

- **Dataset**: Iris (loaded via `sklearn.datasets`)
- **Classes**: Setosa, Versicolor, Virginica
- **Features Used**: Sepal length, Sepal width (for visualization)

## ðŸ“· Demo

![Decision Boundary](https://upload.wikimedia.org/wikipedia/commons/e/e7/Iris_dataset_scatterplot.svg)

> _Note: Image above is a general representation; actual output includes similar decision boundaries generated from your model._
